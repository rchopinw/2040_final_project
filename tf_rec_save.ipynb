{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_rec_save.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaMoG3Kn2iw7"
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import datetime\n",
        "import math\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from functools import reduce\n",
        "import gensim\n",
        "\n",
        "data_files = ['azn', 'biontech', 'jnj', 'moderna', 'novavax', 'pfizer']\n",
        "\n",
        "\n",
        "def save_obj(obj, name):\n",
        "    \"\"\"\n",
        "    :param obj:\n",
        "    :param name:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def load_obj(name):\n",
        "    \"\"\"\n",
        "    :param name:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "def list_major_and_related(dir):\n",
        "    \"\"\"\n",
        "    :param dir:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    files = os.listdir(dir)\n",
        "    major_tweets_f = []\n",
        "    related_tweets_f = []\n",
        "    for fn in files:\n",
        "        if 'related' in fn:\n",
        "            related_tweets_f.append(fn)\n",
        "        else:\n",
        "            major_tweets_f.append(fn)\n",
        "    return major_tweets_f, related_tweets_f\n",
        "\n",
        "\n",
        "def read_file(n):\n",
        "    \"\"\"\n",
        "    :param n:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with open(n, 'rb') as f:\n",
        "        fr = f.readlines()\n",
        "\n",
        "    def decoder(s):\n",
        "        try:\n",
        "            ds = str(s, 'utf_8_sig')\n",
        "        except UnicodeDecodeError:\n",
        "            try:\n",
        "                ds = str(s, 'utf-8')\n",
        "            except UnicodeDecodeError:\n",
        "                try:\n",
        "                    ds = str(s, 'gbk')\n",
        "                except UnicodeDecodeError:\n",
        "                    try:\n",
        "                        ds = str(s, 'GB18030')\n",
        "                    except UnicodeDecodeError:\n",
        "                        ds = str(s, 'gb2312', 'ignore')\n",
        "        return ds\n",
        "\n",
        "    fr = list(map(lambda x: json.loads(decoder(x)), fr))\n",
        "    return fr\n",
        "\n",
        "\n",
        "def process_file(file_dir: str,\n",
        "                 company_name: str,\n",
        "                 abandon: list = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processing the related twitters regarding company_name\n",
        "    :param file_dir: file directory\n",
        "    :param company_name: target company name\n",
        "    :param abandon: the columns to abandon\n",
        "    :return: a pandas dataframe of cleaned data\n",
        "    \"\"\"\n",
        "    if abandon is None:\n",
        "        abandon = ['conversation_id', 'created_at', 'time', 'timezone', 'user_id',\n",
        "                   'username', 'name', 'place', 'mentions', 'link', 'quote_url',\n",
        "                   'thumbnail', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
        "                   'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src', 'trans_dest']\n",
        "    op = read_file(file_dir)\n",
        "    d = pd.DataFrame(op)\n",
        "    d = d.drop_duplicates(subset=['id'])\n",
        "    d = d.drop(columns=abandon)\n",
        "    d = d[d['language'] == 'en'].reset_index(drop=True)\n",
        "    d = d.drop(columns=['language'])\n",
        "    # process date\n",
        "    d['date'] = d['date'].apply(lambda x: x.replace('-', ''))\n",
        "    # process urls:\n",
        "    d['linked_url'] = d['urls'].apply(lambda x: 1 if x else 0)\n",
        "    d = d.drop(columns=['urls'])\n",
        "    # process photos:\n",
        "    d['with_photo'] = d['photos'].apply(lambda x: 1 if x else 0)\n",
        "    d = d.drop(columns=['photos'])\n",
        "    # process retweet\n",
        "    d['retweet'] = d['retweet'].apply(lambda x: 1 if x else 0)\n",
        "    # process video\n",
        "    d['video'] = d['video'].apply(lambda x: 1 if x else 0)\n",
        "    # process date:\n",
        "    d['date'] = d['date'].apply(lambda x: datetime.datetime.strptime(x, '%Y%m%d'))\n",
        "    d['date'] = d['date'].apply(\n",
        "        lambda x: x + datetime.timedelta(days=-1) if x.weekday() == 5 else x + datetime.timedelta(\n",
        "            days=-2) if x.weekday() == 6 else x)\n",
        "    d['date'] = d['date'].apply(lambda x: datetime.datetime.strftime(x, '%Y%m%d'))\n",
        "    # process format:\n",
        "    d['replies_count'] = d['replies_count'].astype('int64')\n",
        "    d['retweets_count'] = d['retweets_count'].astype('int64')\n",
        "    d['likes_count'] = d['likes_count'].astype('int64')\n",
        "    d['retweet'] = d['retweet'].astype('int64')\n",
        "    d['video'] = d['video'].astype('int64')\n",
        "    d['linked_url'] = d['linked_url'].astype('int64')\n",
        "    d['with_photo'] = d['with_photo'].astype('int64')\n",
        "    # group by date\n",
        "    new_df = [dict(zip(sub_df.columns, [sum(sub_df[x])\n",
        "                                        if sub_df[x].dtype == 'int64'\n",
        "                                        else sub_df[x].tolist()\n",
        "                                        for x in sub_df.columns]))\n",
        "              for _, sub_df in d.groupby('date')]\n",
        "    new_df = pd.DataFrame(new_df)\n",
        "    new_df['company_name'] = company_name\n",
        "    new_df['date'] = new_df['date'].apply(lambda x: x[0])\n",
        "    return new_df\n",
        "\n",
        "\n",
        "# process json file for each company\n",
        "def gather_all(dfs: list = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    :param dfs:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if dfs is None:\n",
        "        dfs = data_files\n",
        "    corp_gathered = []\n",
        "    for f in dfs:\n",
        "        mtf, rtf = list_major_and_related('/content/drive/MyDrive/2040 final/2040 final proj/{}'.format(f))\n",
        "        print('...Processing file {}...{}'.format(f, rtf))\n",
        "        corp_file = []\n",
        "        for j in rtf + mtf:\n",
        "            j_d = '/content/drive/MyDrive/2040 final/2040 final proj/{}/'.format(f) + j\n",
        "            corp_file.append(process_file(file_dir=j_d,\n",
        "                                          company_name=f))\n",
        "        corp_file = reduce(lambda x, y: pd.concat([x, y], ignore_index=True), corp_file)\n",
        "        corp_gathered.append(corp_file)\n",
        "    corp_gathered = reduce(lambda x, y: pd.concat([x, y], ignore_index=True), corp_gathered)\n",
        "    corp_gathered = corp_gathered.drop_duplicates(subset=['date', 'company_name'], keep='first')\n",
        "    return corp_gathered\n",
        "\n",
        "\n",
        "def cum_sum(target):\n",
        "    \"\"\"\n",
        "    :param target:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    s = sum(target)\n",
        "    r = list(range(s))\n",
        "    res = []\n",
        "    for i in target:\n",
        "        tmp = r[0:i]\n",
        "        r = r[i:]\n",
        "        res.append(tmp)\n",
        "    return res\n",
        "\n",
        "\n",
        "class ProcessR(object):\n",
        "    def __init__(self,\n",
        "                 text,\n",
        "                 filters='',\n",
        "                 padding='post'):\n",
        "        \"\"\"\n",
        "        :param text:\n",
        "        :param filters:\n",
        "        :param padding:\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "        self.padding = padding\n",
        "        self.filters = filters\n",
        "        print('...{} Documents Loaded...'.format(len(self.text)))\n",
        "        self.docs = self._preprocess_docs()\n",
        "        print('...Language Preprocess Finished...')\n",
        "        self.docs_f, self.tokenizer_f = self._tokenize()\n",
        "        print('...Language Tokenizing Finished...')\n",
        "\n",
        "    def _preprocess_docs(self):\n",
        "        # Including steps: removing punctuations,\n",
        "        #                  removing duplicated white spaces,\n",
        "        #                  remove numbers,\n",
        "        #                  remove stopwords,\n",
        "        #                  stemming the text.\n",
        "        return gensim.parsing.preprocessing.preprocess_documents(self.text)\n",
        "\n",
        "    def _tokenize(self):\n",
        "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=self.filters)\n",
        "        lang_tokenizer.fit_on_texts(self.docs)\n",
        "        tensor = lang_tokenizer.texts_to_sequences(self.docs)\n",
        "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding=self.padding)\n",
        "        return tensor.tolist(), lang_tokenizer\n",
        "\n",
        "\n",
        "def generate_tf_rec(processor: ProcessR,\n",
        "                    f_m : pd.DataFrame,\n",
        "                    sequence_length: int,\n",
        "                    map_relation: dict,\n",
        "                    num_per_day: int,\n",
        "                    n_repeat: int,\n",
        "                    test_size: float):\n",
        "    \"\"\"\n",
        "    :param test_size:\n",
        "    :param n_repeat:\n",
        "    :param num_per_day:\n",
        "    :param processor:\n",
        "    :param news_info:\n",
        "    :param stock_info:\n",
        "    :param map_relation:\n",
        "    :param sequence_length:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    x, y = df_process(f_m)\n",
        "    f_m['target_variation'] = y\n",
        "    for company, sub_df in f_m.groupby('company_name'):\n",
        "        print('Processing company {}'.format(company))\n",
        "        train_writer = tf.io.TFRecordWriter(path='/content/drive/MyDrive/2040 final/2040 final proj/{}_train.tfrecords'.format(company))\n",
        "        validation_writer = tf.io.TFRecordWriter(path='/content/drive/MyDrive/2040 final/2040 final proj/{}_validation.tfrecords'.format(company))\n",
        "        tmp = sub_df.sort_values(by=['date'], ascending=True)\n",
        "        sub_x = x[tmp.index, :]\n",
        "        tmp = tmp.reset_index(drop=True)\n",
        "        train_index = range(math.floor((tmp.shape[0] - sequence_length + 1)*(1-test_size)))\n",
        "        for i in range(tmp.shape[0] - sequence_length):\n",
        "            fluctuation_norm = tmp.iloc[i+sequence_length, :]['target_variation']\n",
        "            sequence_date = tmp.iloc[i:i+sequence_length, :]['date']\n",
        "            sequence_x = sub_x[i:i+sequence_length, :]\n",
        "            for _ in range(random.randint(n_repeat - 10, n_repeat + 10)):\n",
        "                sub_rec = []\n",
        "                for d in sequence_date:\n",
        "                    word_v = [processor.docs_f[x] for x in map_relation['{}_{}'.format(d, company)]]\n",
        "                    word_v = [word_v[x] for x in [random.randint(0, len(word_v)-1) for _ in range(num_per_day)]]\n",
        "                    sub_rec.append(word_v)\n",
        "                sub_rec = np.array(sub_rec).astype('int64')\n",
        "                sequence_x = sequence_x.astype('float32')\n",
        "                example = tf.train.Example(features=tf.train.Features(\n",
        "                    feature={\n",
        "                        'x1': tf.train.Feature(bytes_list=tf.train.BytesList(value=[sub_rec.tobytes()])),\n",
        "                        'x2': tf.train.Feature(bytes_list=tf.train.BytesList(value=[sequence_x.tobytes()])),\n",
        "                        'y': tf.train.Feature(float_list=tf.train.FloatList(value=[fluctuation_norm]))\n",
        "                    }\n",
        "                ))\n",
        "                if i in train_index:\n",
        "                    train_writer.write(example.SerializeToString())\n",
        "                else:\n",
        "                    validation_writer.write(example.SerializeToString())\n",
        "        train_writer.close()\n",
        "        validation_writer.close()\n",
        "    print('Finished')\n",
        "\n",
        "\n",
        "def df_process(df,\n",
        "               drop_columns=None,\n",
        "               norm=True,\n",
        "               norm_columns=None,\n",
        "               norm_approach='min-max'):\n",
        "    \"\"\"\n",
        "    :param df:\n",
        "    :param drop_columns:\n",
        "    :param norm:\n",
        "    :param norm_columns:\n",
        "    :param norm_approach:\n",
        "    :param predict_time:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if drop_columns is None:\n",
        "        drop_columns = ['open', 'high', 'low', 'close',\n",
        "                        'adjclose', 'volume', 'company_name', 'date',\n",
        "                        'hashtags', 'cashtags', 'retweet']\n",
        "    df = df.drop(columns=drop_columns)\n",
        "    df = df.dropna()\n",
        "    if norm:\n",
        "        if norm_columns is None:\n",
        "            norm_columns = ['replies_count', 'retweets_count',\n",
        "                            'likes_count',\n",
        "                            'video', 'linked_url', 'with_photo', 'target_variation']\n",
        "        if norm_approach == 'min-max':\n",
        "            s = MinMaxScaler()\n",
        "        else:\n",
        "            s = StandardScaler()\n",
        "        s.fit(df[norm_columns])\n",
        "        s_f = np.array(s.transform(df[norm_columns]))\n",
        "    else:\n",
        "        s_f = np.array(df)\n",
        "    y = s_f[:, -1]\n",
        "    x = s_f[:, :-1]\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfT1R0LW_RDC",
        "outputId": "0c53f69a-24c4-40e5-95d8-0420fcdd2b86"
      },
      "source": [
        "gather_all().to_csv('/content/drive/MyDrive/2040 final/2040 final proj/data_op.csv', encoding='utf_8_sig', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...Processing file azn...['azn_related_202006202008.json', 'azn_related_201912202006.json', 'azn_related_20160401_20191201.json', 'azn_related_202008202101.json', 'azn_related_202101202104.json']\n",
            "...Processing file biontech...['biontech_related_201912202006.json', 'bntx_related_20191010_20191201.json', 'biontech_related_202006202008.json', 'biontech_related_202008202010.json', 'biontech_related_202010202012.json', 'biontech_related_202012202102.json', 'biontech_related_202102202104.json']\n",
            "...Processing file jnj...['jnj_related_201912202006.json', 'jnj_related_20160401_20191201.json', 'jnj_related_202006202101.json', 'jnj_related_202101202104.json']\n",
            "...Processing file moderna...['moderna_related_201912202006.json', 'mrna_related_20181207_20191201.json', 'moderna_related_202006202010.json', 'moderna_related_202010202102.json', 'moderna_related_202102202104.json']\n",
            "...Processing file novavax...['novavax_related_201912202006.json', 'nvax_related_20160401_20191201.json', 'novavax_related_202006202010.json', 'novavax_related_202010202102.json', 'novavax_related_202102202104.json']\n",
            "...Processing file pfizer...['pfe_related_201912202006.json', 'pfe_related_20160401_20191201.json', 'pfizer_related_0607.json', 'pfizer_related_0708.json', 'pfizer_related_0809.json', 'pfizer_related_09010.json', 'pfizer_related_1011.json', 'pfizer_related_1112.json', 'pfizer_related_20201202102.json', 'pfizer_related_20210202104.json']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpfkGThpAhPO"
      },
      "source": [
        "related_corp_tweets = pd.read_csv('/content/drive/MyDrive/2040 final/2040 final proj/data_op.csv', encoding='utf_8_sig')\n",
        "stock_price = pd.read_csv('/content/drive/MyDrive/2040 final/2040 final proj/stockprice_201604_202006.csv', encoding='utf_8_sig', index_col=0)\n",
        "\n",
        "related_corp_tweets['date'] = related_corp_tweets['date'].astype('str')\n",
        "stock_price['date'] = stock_price['date'].astype('str')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuE9ZmGf-kCE"
      },
      "source": [
        "# mapping date to tweet\n",
        "tweet_stock_merge = pd.merge(related_corp_tweets, stock_price, on=['company_name', 'date'], how='right')\n",
        "tweet_stock_merge = tweet_stock_merge.dropna().reset_index(drop=True)\n",
        "date_text_related = dict(zip(tweet_stock_merge.date + '_' + tweet_stock_merge.company_name, cum_sum([len(eval(x)) for x in tweet_stock_merge.tweet.tolist()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yeUIr3sBIuB",
        "outputId": "7e91193d-4c51-4147-d93d-5eb43627e4e7"
      },
      "source": [
        "# format the texts\n",
        "text_related = sum([eval(x) for x in tweet_stock_merge.tweet.tolist()], [])\n",
        "\n",
        "related_pr = ProcessR(text_related)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...597314 Documents Loaded...\n",
            "...Language Preprocess Finished...\n",
            "...Language Tokenizing Finished...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6MFjCEZVjsz",
        "outputId": "4bb629c4-4d61-49b5-f11a-6fe9f7baf140"
      },
      "source": [
        "generate_tf_rec(processor=related_pr,\n",
        "                f_m=tweet_stock_merge,\n",
        "                sequence_length=10,\n",
        "                map_relation=date_text_related,\n",
        "                num_per_day=50,\n",
        "                n_repeat=20,\n",
        "                test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing company azn\n",
            "Processing company biontech\n",
            "Processing company jnj\n",
            "Processing company moderna\n",
            "Processing company novavax\n",
            "Processing company pfizer\n",
            "Finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}